\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, graphicx, hyperref}
\usepackage{enumitem}
\setlist{nosep}
\usepackage[margin=1in]{geometry}

\title{LLM-Assisted Data Extraction (Human-in-the-Loop)}
\author{}
\date{}

\begin{document}
\maketitle

\noindent \textbf{Note on data.} This problem set uses \textbf{synthetic} (simulated) text snippets provided in the code tutorial. The goal is to practice structured extraction with LLMs, auditing, and evaluation---not to make substantive claims about real events.

\bigskip
\noindent \textbf{LLM requirement.} You must use a \textbf{free local LLM}. You may use either:
\begin{itemize}
    \item \textbf{Ollama} (e.g., \texttt{llama3.1:8b}, \texttt{mistral:7b-instruct}), or
    \item A \textbf{Hugging Face Transformers} model run locally (e.g., \texttt{google/flan-t5-small}).
\end{itemize}
Do \emph{not} use a paid API.

\section*{Start Off: Verifying Your Environment}

\begin{enumerate}
\item \textbf{Environment check (required).}  

Submit proof that you successfully ran the full tutorial code on your machine. You may submit \emph{one} of the following:

\begin{itemize}
  \item A screenshot or text file showing console output that includes:
  \begin{itemize}
      \item the first few rows of the extracted dataset,
      \item review flag counts, and
      \item the classification report.
  \end{itemize}
  \item A screenshot of your \texttt{outputs/} directory showing generated CSV files (e.g., \texttt{extractions\_raw.csv}, \texttt{extractions\_with\_flags.csv}, \texttt{human\_audit\_sheet.csv}).
  \item A Git commit (hash or screenshot) that includes at least one generated output file.
  \item A short log file (e.g., \texttt{run\_log.txt}) containing printed diagnostics and evaluation metrics.
\end{itemize}

Your submission must clearly demonstrate that the full pipeline ran successfully.
\end{enumerate}

\section*{Conceptual Questions}

Please write three to ten sentence explanations for each of the following questions. \textbf{You are only required to answer ONE of the two questions below.}

\bigskip

\begin{enumerate}
\setcounter{enumi}{1}

\item Explain why \textbf{schema-constrained} extraction (structured JSON fields with explicit missingness rules) can reduce hallucination relative to free-form summarization. Then explain one limitation: a way the model can still produce systematically wrong extractions even when it outputs ``valid'' JSON.

\item Human-in-the-loop extraction requires evaluation. Explain why \textbf{spot-audits} and \textbf{precision/recall}-style evaluation are both needed. In your answer, define:
\begin{itemize}
    \item one failure mode that would be missed by evaluating only on a small gold set, and
    \item one failure mode that would be missed by auditing only a small random sample.
\end{itemize}

\end{enumerate}

\section*{Applied Exercises}

Use the code in the weekâ€™s tutorial and lecture slides to answer the following questions. \textbf{You are only required to answer TWO of the three questions below.}

\bigskip

\begin{enumerate}
\setcounter{enumi}{3}

\item \textbf{Run structured extraction using a free local LLM.}

Starting from the provided script (\texttt{llm\_human.py}):

\begin{itemize}
  \item Use a \textbf{free local LLM} (Ollama or Hugging Face).
  \item Keep the \texttt{EventExtraction} schema.
  \item Require the model to output a single JSON object that matches the schema.
  \item Run extraction for all documents.
  \item Save the output table to \texttt{outputs/extractions\_raw.csv}.
\end{itemize}

Because local models are not perfectly reliable at producing valid JSON, you must:

\begin{itemize}
  \item Log the raw model output,
  \item Report the number (or share) of parse failures (if any), and
  \item Explain briefly how you handled invalid JSON outputs.
\end{itemize}

In your submission, report:
\begin{itemize}
  \item Which model you used (e.g., \texttt{llama3.1:8b}, \texttt{flan-t5-small}),
  \item The exact prompt you used.
\end{itemize}

\bigskip

\item \textbf{Uncertainty flags + audit sheet (human-in-the-loop).}

Using your extracted dataset:

\begin{itemize}
  \item Create at least \textbf{four} mechanical review flags (examples: low confidence, missing date, missing country, \texttt{geo\_precision = unknown}, empty actors list, parse failure).
  \item Create a \textbf{single audit sheet CSV} that includes:
  \begin{enumerate}
    \item the raw text,
    \item the extracted fields,
    \item the evidence quotes, and
    \item blank columns for human corrections and failure-mode tags.
  \end{enumerate}
  \item Fill out the audit sheet for at least \textbf{five} documents.
  \item For any incorrect extraction, tag a failure mode (e.g., \texttt{date\_missing}, \texttt{location\_vague}, \texttt{event\_type\_wrong}, \texttt{actor\_hallucination}, \texttt{parse\_failure}).
\end{itemize}

Report two audit statistics:
\begin{enumerate}
  \item the share of audited rows marked correct, and
  \item the most common failure mode (a small frequency table is sufficient).
\end{enumerate}

\bigskip

\item \textbf{Evaluation + prompt iteration.}

Using the small gold set in the tutorial:

\begin{itemize}
  \item Compute and report a classification report (precision/recall/F1) for \texttt{event\_type}.
  \item Create \textbf{two} prompt variants (e.g., different missingness instructions, stronger evidence requirements, shorter taxonomy explanation).
  \item Re-run extraction and evaluation for both prompts.
  \item Present a \textbf{small table} comparing at least:
  \begin{enumerate}
    \item macro-F1,
    \item accuracy, and
    \item number of items flagged for human review.
  \end{enumerate}
\end{itemize}

In 6--10 sentences, defend which prompt you would use in a larger project. Your answer must reference both:
\begin{itemize}
    \item quantitative evaluation results, and
    \item auditing considerations.
\end{itemize}

\end{enumerate}

\section*{Challenge Question (Optional --- if you finish early)}

\begin{enumerate}
\setcounter{enumi}{6}

\item Make the pipeline more robust to long documents or ambiguous text. Choose \textbf{ONE} option:

\begin{enumerate}
  \item \textbf{Chunking.} Split each document into 2--3 chunks, run extraction per chunk, and write a rule-based aggregation step that outputs one final record (e.g., choose the highest-confidence chunk, union actors, keep the most specific location). In 2--4 sentences, explain how chunking changes failure modes.

  \item \textbf{Abstention policy.} Add an explicit abstention rule: if the model is unsure, it must set \texttt{event\_type = other} and add an uncertainty flag. Compare:
  \begin{itemize}
      \item event-type macro-F1, and
      \item the review queue size,
  \end{itemize}
  before vs.\ after abstention. Interpret the trade-off in 5--8 sentences.
\end{enumerate}

\end{enumerate}

\end{document}